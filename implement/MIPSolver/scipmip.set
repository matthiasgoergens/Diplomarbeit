# branching score function ('s'um, 'p'roduct)
# [type: char, range: {sp}, default: p]
branching/scorefunc = p

# branching score factor to weigh downward and upward gain prediction in sum score function
# [type: real, range: [0,1], default: 0.167]
branching/scorefac = 0.167

# should branching on binary variables be preferred?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
branching/preferbinary = FALSE

# should conflict analysis be enabled?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/enable = TRUE

# should propagation conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/useprop = FALSE

# should infeasible LP conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/useinflp = FALSE

# should bound exceeding LP conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/useboundlp = FALSE

# should infeasible/bound exceeding strong branching conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/usesb = FALSE

# should pseudo solution conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/usepseudo = FALSE

# maximal fraction of variables involved in a conflict constraint
# [type: real, range: [0,1.79769313486232e+308], default: 0.1]
conflict/maxvarsfac = 0.02

# minimal absolute maximum of variables involved in a conflict constraint
# [type: int, range: [0,2147483647], default: 30]
conflict/minmaxvars = 30

# maximal number of LP resolving loops during conflict analysis (-1: no limit)
# [type: int, range: [-1,2147483647], default: 2]
conflict/maxlploops = 0

# maximal number of LP iterations in each LP resolving loop (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
conflict/lpiterations = 0

# number of depth levels up to which first UIP's are used in conflict analysis (-1: use All-FirstUIP rule)
# [type: int, range: [-1,2147483647], default: -1]
conflict/fuiplevels = -1

# maximal number of intermediate conflict constraints generated in conflict graph (-1: use every intermediate constraint)
# [type: int, range: [-1,2147483647], default: -1]
conflict/interconss = 1

# number of depth levels up to which UIP reconvergence constraints are generated (-1: generate reconvergence constraints in all depth levels)
# [type: int, range: [-1,2147483647], default: -1]
conflict/reconvlevels = 1

# maximal number of conflict constraints accepted at an infeasible node (-1: use all generated conflict constraints)
# [type: int, range: [-1,2147483647], default: 10]
conflict/maxconss = 10

# should binary conflicts be preferred?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/preferbinary = FALSE

# should conflict constraints be generated that are only valid locally?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/allowlocal = TRUE

# should conflict constraints be attached only to the local subtree where they can be useful?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/settlelocal = TRUE

# should earlier nodes be repropagated in order to replace branching decisions by deductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/repropagate = TRUE

# should constraints be kept for repropagation even if they are too long?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/keepreprop = TRUE

# should the conflict constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/dynamic = TRUE

# should the conflict's relaxations be subject to LP aging and cleanup?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/removable = TRUE

# score factor for depth level in bound relaxation heuristic of LP analysis
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 1]
conflict/depthscorefac = 1

# factor to decrease importance of variables' earlier conflict scores
# [type: real, range: [1e-06,1], default: 0.98]
conflict/scorefac = 0.98

# number of successful conflict analysis calls that trigger a restart (0: disable conflict restarts)
# [type: int, range: [0,2147483647], default: 0]
conflict/restartnum = 0

# factor to increase restartnum with after each restart
# [type: real, range: [0,1.79769313486232e+308], default: 1.5]
conflict/restartfac = 1.5

# maximum age an unnecessary constraint can reach before it is deleted (0: dynamic, -1: keep all constraints)
# [type: int, range: [-1,2147483647], default: 0]
constraints/agelimit = 0

# age of a constraint after which it is marked obsolete (0: dynamic, -1 do not mark constraints obsolete)
# [type: int, range: [-1,2147483647], default: -1]
constraints/obsoleteage = -1

# should enforcement of pseudo solution be disabled?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/disableenfops = FALSE

# verbosity level of output
# [type: int, range: [0,5], default: 4]
display/verblevel = 4

# maximal number of characters in a node information line
# [type: int, range: [0,2147483647], default: 139]
display/width = 139

# frequency for displaying node information lines
# [type: int, range: [-1,2147483647], default: 100]
display/freq = 100

# frequency for displaying header lines (every n'th node information line)
# [type: int, range: [-1,2147483647], default: 15]
display/headerfreq = 15

# should the LP solver display status messages?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
display/lpinfo = FALSE

# maximal time in seconds to run
# [type: real, range: [0,1.79769313486232e+308], default: 1e+20]
limits/time = 1e+20

# maximal number of nodes to process (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
limits/nodes = -1

# solving stops, if the given number of nodes was processed since the last improvement of the primal solution value (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
limits/stallnodes = -1

# maximal memory usage in MB; reported memory usage is lower than real memory usage!
# [type: real, range: [0,1.79769313486232e+308], default: 1e+20]
limits/memory = 1e+20

# solving stops, if the relative gap = |(primalbound - dualbound)/dualbound| is below the given value
# [type: real, range: [0,1.79769313486232e+308], default: 0]
limits/gap = 0

# solving stops, if the absolute gap = |primalbound - dualbound| is below the given value
# [type: real, range: [0,1.79769313486232e+308], default: 0]
limits/absgap = 0

# solving stops, if the given number of solutions were found (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
limits/solutions = -1

# solving stops, if the given number of solution improvements were found (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
limits/bestsol = -1

# maximal number of solutions to store in the solution storage
# [type: int, range: [1,2147483647], default: 100]
limits/maxsol = 100

# frequency for solving LP at the nodes (-1: never; 0: only root LP)
# [type: int, range: [-1,2147483647], default: 1]
lp/solvefreq = 1

# maximal depth for solving LP at the nodes (-1: no depth limit)
# [type: int, range: [-1,2147483647], default: -1]
lp/solvedepth = -1

# LP algorithm for solving initial LP relaxations (automatic 's'implex, 'p'rimal simplex, 'd'ual simplex, 'b'arrier, barrier with 'c'rossover)
# [type: char, range: {spdbc}, default: s]
lp/initalgorithm = s

# LP algorithm for resolving LP relaxations if a starting basis exists (automatic 's'implex, 'p'rimal simplex, 'd'ual simplex, 'b'arrier, barrier with 'c'rossover)
# [type: char, range: {spdbc}, default: s]
lp/resolvealgorithm = s

# LP pricing strategy ('l'pi default, 'a'uto, 'f'ull pricing, 'p'artial, 's'teepest edge pricing, 'q'uickstart steepest edge pricing, 'd'evex pricing)
# [type: char, range: {lafpsqd}, default: l]
lp/pricing = s

# maximum age a dynamic column can reach before it is deleted from the LP (-1: don't delete columns due to aging)
# [type: int, range: [-1,2147483647], default: 10]
lp/colagelimit = 10

# maximum age a dynamic row can reach before it is deleted from the LP (-1: don't delete rows due to aging)
# [type: int, range: [-1,2147483647], default: 10]
lp/rowagelimit = 10

# should new non-basic columns be removed after LP solving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/cleanupcols = FALSE

# should new non-basic columns be removed after root LP solving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/cleanupcolsroot = FALSE

# should new basic rows be removed after LP solving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/cleanuprows = TRUE

# should new basic rows be removed after root LP solving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/cleanuprowsroot = TRUE

# should LP solver's return status be checked for stability?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/checkstability = TRUE

# should LP solutions be checked, resolving LP when numerical troubles occur?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/checkfeas = TRUE

# should FASTMIP setting of LP solver be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/fastmip = TRUE

# should scaling of LP solver be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/scaling = TRUE

# should presolving of LP solver be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/presolving = TRUE

# fraction of maximal memory usage resulting in switch to memory saving mode
# [type: real, range: [0,1], default: 0.8]
memory/savefac = 0.8

# memory growing factor for dynamically allocated arrays
# [type: real, range: [1,10], default: 1.2]
memory/arraygrowfac = 1.2

# initial size of dynamically allocated arrays
# [type: int, range: [0,2147483647], default: 4]
memory/arraygrowinit = 4

# memory growing factor for tree array
# [type: real, range: [1,10], default: 2]
memory/treegrowfac = 2

# initial size of tree array
# [type: int, range: [0,2147483647], default: 65536]
memory/treegrowinit = 65536

# memory growing factor for path array
# [type: real, range: [1,10], default: 2]
memory/pathgrowfac = 2

# initial size of path array
# [type: int, range: [0,2147483647], default: 256]
memory/pathgrowinit = 256

# should the CTRL-C interrupt be caught by SCIP?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/catchctrlc = TRUE

# child selection rule ('d'own, 'u'p, 'p'seudo costs, 'i'nference, 'l'p value, 'r'oot LP value difference, 'h'brid inference/root LP value difference)
# [type: char, range: {dupilrh}, default: h]
nodeselection/childsel = h

# values larger than this are considered infinity
# [type: real, range: [10000000000,1e+98], default: 1e+20]
numerics/infinity = 1e+20

# absolute values smaller than this are considered zero
# [type: real, range: [1e-20,0.001], default: 1e-09]
numerics/epsilon = 1e-09

# absolute values of sums smaller than this are considered zero
# [type: real, range: [1e-17,0.001], default: 1e-06]
numerics/sumepsilon = 1e-06

# LP feasibility tolerance for constraints
# [type: real, range: [1e-17,0.001], default: 1e-06]
numerics/feastol = 1e-06

# LP feasibility tolerance for reduced costs
# [type: real, range: [1e-17,0.001], default: 1e-09]
numerics/dualfeastol = 1e-09

# LP convergence tolerance used in barrier algorithm
# [type: real, range: [1e-17,0.001], default: 1e-10]
numerics/barrierconvtol = 1e-10

# minimal relative improve for strengthening bounds
# [type: real, range: [1e-17,1e+98], default: 0.05]
numerics/boundstreps = 0.0001

# minimal variable distance value to use for branching pseudo cost updates
# [type: real, range: [1e-17,1], default: 0.1]
numerics/pseudocosteps = 0.1

# minimal objective distance value to use for branching pseudo cost updates
# [type: real, range: [0,1.79769313486232e+308], default: 0.0001]
numerics/pseudocostdelta = 0.0001

# maximal number of presolving rounds (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
presolving/maxrounds = -1

# abort presolve, if at most this fraction of the problem was changed in last presolve round
# [type: real, range: [0,1], default: 0.0001]
presolving/abortfac = 0.0001

# maximal number of restarts (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
presolving/maxrestarts = -1

# fraction of integer variables that were fixed in the root node triggering a restart with preprocessing after root node evaluation
# [type: real, range: [0,1], default: 0.05]
presolving/restartfac = 0

# fraction of integer variables that were fixed in the root node triggering an immediate restart with preprocessing
# [type: real, range: [0,1], default: 0.2]
presolving/immrestartfac = 0.2

# fraction of integer variables that were globally fixed during the solving process triggering a restart with preprocessing
# [type: real, range: [0,1], default: 1]
presolving/subrestartfac = 1

# minimal fraction of integer variables removed after restart to allow for an additional restart
# [type: real, range: [0,1], default: 0.1]
presolving/restartminred = 0.1

# maximal number of variables priced in per pricing round
# [type: int, range: [1,2147483647], default: 100]
pricing/maxvars = 100

# maximal number of priced variables at the root node
# [type: int, range: [1,2147483647], default: 2000]
pricing/maxvarsroot = 2000

# pricing is aborted, if fac * pricing/maxvars pricing candidates were found
# [type: real, range: [1,1.79769313486232e+308], default: 2]
pricing/abortfac = 2

# maximal number of propagation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
propagating/maxrounds = 100

# maximal number of propagation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 1000]
propagating/maxroundsroot = 1000

# should propagation be aborted immediately? setting this to FALSE could help conflict analysis to produce more conflict constraints
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/abortoncutoff = TRUE

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separation (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
separating/maxbounddist = 0.2

# minimal efficacy for a cut to enter the LP
# [type: real, range: [0,1e+98], default: 0.05]
separating/minefficacy = 0.05

# minimal efficacy for a cut to enter the LP in the root node
# [type: real, range: [0,1e+98], default: 0.01]
separating/minefficacyroot = 0.01

# minimal orthogonality for a cut to enter the LP
# [type: real, range: [0,1], default: 0.5]
separating/minortho = 0.5

# minimal orthogonality for a cut to enter the LP in the root node
# [type: real, range: [0,1], default: 0.5]
separating/minorthoroot = 0.5

# factor to scale objective parallelism of cut in separation score calculation
# [type: real, range: [0,1e+98], default: 0.0001]
separating/objparalfac = 0.2

# factor to scale orthogonality of cut in separation score calculation (0.0 to disable orthogonality calculation)
# [type: real, range: [0,1e+98], default: 1]
separating/orthofac = 1

# row norm to use for efficacy calculation ('e'uclidean, 'm'aximum, 's'um, 'd'iscrete)
# [type: char, range: {emsd}, default: e]
separating/efficacynorm = e

# maximal number of runs for which separation is enabled (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/maxruns = -1

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/maxrounds = 5

# maximal number of separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/maxroundsroot = -1

# maximal number of separation rounds in the root node of a subsequent run (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 1]
separating/maxroundsrootsubrun = 1

# maximal additional number of separation rounds in subsequent price-and-cut loops (-1: no additional restriction)
# [type: int, range: [-1,2147483647], default: 1]
separating/maxaddrounds = 1

# maximal number of consecutive separation rounds without objective or integrality improvement (-1: no additional restriction)
# [type: int, range: [-1,2147483647], default: 5]
separating/maxstallrounds = 5

# maximal number of cuts separated per separation round (0: disable local separation)
# [type: int, range: [0,2147483647], default: 100]
separating/maxcuts = 100

# maximal number of separated cuts at the root node (0: disable root node separation)
# [type: int, range: [0,2147483647], default: 2000]
separating/maxcutsroot = 2000

# maximum age a cut can reach before it is deleted from the global cut pool, or -1 to keep all cuts
# [type: int, range: [-1,2147483647], default: 100]
separating/cutagelimit = 100

# separation frequency for the global cut pool (-1: disable global cut pool, 0: only separate pool at the root)
# [type: int, range: [-1,2147483647], default: 0]
separating/poolfreq = 0

# default clock type (1: CPU user seconds, 2: wall clock time)
# [type: int, range: [1,2], default: 1]
timing/clocktype = 1

# is timing enabled?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
timing/enabled = TRUE

# name of the VBC Tool output file, or - if no VBC Tool output should be created
# [type: string, default: "-"]
vbc/filename = "-"

# should the real solving time be used instead of a time step counter in VBC output?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
vbc/realtime = TRUE

# priority of conflict handler <linear>
# [type: int, range: [-2147483648,2147483647], default: -1000000]
conflict/linear/priority = -1000000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/linear/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/linear/propfreq = 5

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/linear/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/linear/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/delaypresol = FALSE

# multiplier on propagation frequency, how often the bounds are tightened (-1: never, 0: only at root)
# [type: int, range: [-1,2147483647], default: 1]
constraints/linear/tightenboundsfreq = 1

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
constraints/linear/maxrounds = 5

# maximal number of separation rounds per node in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
constraints/linear/maxroundsroot = -1

# maximal number of cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
constraints/linear/maxsepacuts = 50

# maximal number of cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 200]
constraints/linear/maxsepacutsroot = 200

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/presolpairwise = FALSE

# should hash table be used for detecting redundant constraints in advance
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/presolusehashing = TRUE

# maximal allowed relative gain in maximum norm for constraint aggregation (0.0: disable constraint aggregation)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
constraints/linear/maxaggrnormscale = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for separating knapsack cardinality cuts
# [type: real, range: [0,1], default: 0]
constraints/linear/maxcardbounddist = 0

# should all constraints be subject to cardinality cut generation instead of only the ones with non-zero dual value?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/separateall = FALSE

# should presolving search for aggregations in equations
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/aggregatevariables = TRUE

# should presolving try to simplify inequalities
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/simplifyinequalities = FALSE

# should dual presolving steps be preformed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/dualpresolving = TRUE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/and/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/and/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/and/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/and/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/and/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/and/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/and/delaypresol = FALSE

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/and/presolpairwise = FALSE

# should the lp relaxation be in the initial LP (0: FALSE, 1: auto, 2: TRUE)
# [type: int, range: [0,2], default: 1]
constraints/and/initiallp = 1

# should the "and" constraint get linearized and removed (in presolving)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/and/linearize = FALSE

# priority of conflict handler <bounddisjunction>
# [type: int, range: [-2147483648,2147483647], default: -3000000]
conflict/bounddisjunction/priority = -3000000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/bounddisjunction/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/bounddisjunction/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/bounddisjunction/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/bounddisjunction/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/bounddisjunction/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/bounddisjunction/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/bounddisjunction/delaypresol = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/conjunction/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/conjunction/propfreq = -1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/conjunction/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/conjunction/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/conjunction/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/conjunction/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/conjunction/delaypresol = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/countsols/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/countsols/propfreq = -1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/countsols/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
constraints/countsols/maxprerounds = 0

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/countsols/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/countsols/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/countsols/delaypresol = FALSE

# should the sparse solution test be turned on?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/countsols/sparsetest = TRUE

# is it allowed to discard solutions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/countsols/discardsols = TRUE

# is the constraint handler active?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/countsols/active = FALSE

# should the solutions be collected?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/countsols/collect = FALSE

# display activation status of display column <sols> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 0]
display/sols/active = 0

# display activation status of display column <feasST> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 0]
display/feasST/active = 0

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/propfreq = -1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/eagerfreq = -1

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/integral/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/integral/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/integral/delaypresol = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/knapsack/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/knapsack/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/knapsack/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/knapsack/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/delaypresol = FALSE

# enable linear upgrading for constraint handler <knapsack>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/knapsack = TRUE

# multiplier on separation frequency, how often cardinality cuts are separated (-1: never, 0: only at root)
# [type: int, range: [-1,2147483647], default: 1]
constraints/knapsack/sepacardfreq = 1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for separating knapsack cardinality cuts
# [type: real, range: [0,1], default: 0]
constraints/knapsack/maxcardbounddist = 0

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
constraints/knapsack/maxrounds = 5

# maximal number of separation rounds per node in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
constraints/knapsack/maxroundsroot = -1

# maximal number of cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
constraints/knapsack/maxsepacuts = 50

# maximal number of cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 200]
constraints/knapsack/maxsepacutsroot = 200

# maximal number of cardinality inequalities lifted per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
constraints/knapsack/maxnumcardlift = -1

# should disaggregation of knapsack constraints be allowed in preprocessing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/disaggregation = TRUE

# priority of conflict handler <logicor>
# [type: int, range: [-2147483648,2147483647], default: 800000]
conflict/logicor/priority = 800000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/logicor/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/logicor/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/logicor/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/logicor/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/delaypresol = FALSE

# enable linear upgrading for constraint handler <logicor>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/logicor = TRUE

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/presolpairwise = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/or/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/or/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/or/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/or/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/or/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/or/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/or/delaypresol = FALSE

# priority of conflict handler <setppc>
# [type: int, range: [-2147483648,2147483647], default: 700000]
conflict/setppc/priority = 700000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/setppc/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/setppc/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/setppc/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/setppc/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/delaypresol = FALSE

# enable linear upgrading for constraint handler <setppc>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/setppc = TRUE

# number of children created in pseudo branching (0: disable pseudo branching)
# [type: int, range: [0,2147483647], default: 2]
constraints/setppc/npseudobranches = 2

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/presolpairwise = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/SOS1/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/SOS1/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/SOS1/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/SOS1/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS1/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS1/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS1/delaypresol = FALSE

# Use SOS1 branching in enforcing (otherwise leave decision to branching rules)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/SOS1/branchSOS = TRUE

# Branch on SOS constraint with most number of nonzeros?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS1/branchNonzeros = FALSE

# Branch on SOS cons. with highest nonzero-variable weight for branching (needs branchNonzeros = false)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS1/branchWeight = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/SOS2/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/SOS2/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/SOS2/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/SOS2/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS2/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS2/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/SOS2/delaypresol = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/varbound/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/varbound/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/varbound/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/varbound/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/varbound/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/varbound/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/varbound/delaypresol = FALSE

# enable linear upgrading for constraint handler <varbound>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/varbound = TRUE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/xor/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/xor/propfreq = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/xor/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/xor/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/xor/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/xor/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/xor/delaypresol = FALSE

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/xor/presolpairwise = FALSE

# should model constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/cnfreader/dynamicconss = TRUE

# should columns be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/cnfreader/dynamiccols = FALSE

# should rows be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/cnfreader/dynamicrows = FALSE

# should model constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/lpreader/dynamicconss = TRUE

# should columns be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/lpreader/dynamiccols = FALSE

# should rows be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/lpreader/dynamicrows = FALSE

# should model constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/mpsreader/dynamicconss = TRUE

# should columns be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/mpsreader/dynamiccols = FALSE

# should rows be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/mpsreader/dynamicrows = FALSE

# should model constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/opbreader/dynamicconss = TRUE

# should columns be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/opbreader/dynamiccols = FALSE

# should rows be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/opbreader/dynamicrows = FALSE

# should the non linear constraint be separated during LP processing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/opbreader/nlcseparate = TRUE

# should the constraint be propagated during node processing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/opbreader/nlcpropagate = TRUE

# should the non linear constraints be removable?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/opbreader/nlcremovable = TRUE

# should the coloring values be relativ or absolute
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/ppmreader/rgbrelativ = TRUE

# should the output format be binary(P6) (otherwise plain(P3) format)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/ppmreader/rgbascii = TRUE

# splitting coefficients in this number of intervals
# [type: int, range: [3,16], default: 3]
reading/ppmreader/coefficientlimit = 3

# maximal color value
# [type: int, range: [0,255], default: 160]
reading/ppmreader/rgblimit = 160

# should columns be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/zplreader/dynamiccols = FALSE

# should the current directory be changed to that of the ZIMPL file before parsing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/zplreader/changedir = TRUE

# additional parameter string passed to the ZIMPL parser (or - for no additional parameters)
# [type: string, default: "-"]
reading/zplreader/parameters = "-"

# priority of presolver <dualfix>
# [type: int, range: [-536870912,536870911], default: 8000000]
presolving/dualfix/priority = 100000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/dualfix/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/dualfix/delay = FALSE

# priority of presolver <implics>
# [type: int, range: [-536870912,536870911], default: -10000]
presolving/implics/priority = -10000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/implics/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/implics/delay = FALSE

# priority of presolver <inttobinary>
# [type: int, range: [-536870912,536870911], default: 7000000]
presolving/inttobinary/priority = 8000000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/inttobinary/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/inttobinary/delay = FALSE

# priority of presolver <probing>
# [type: int, range: [-536870912,536870911], default: -100000]
presolving/probing/priority = -100000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/probing/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
presolving/probing/delay = TRUE

# maximal number of runs, probing participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 1]
presolving/probing/maxruns = 1

# maximal number of propagation rounds in probing subproblems (-1: no limit, 0: auto)
# [type: int, range: [-1,2147483647], default: -1]
presolving/probing/proprounds = -1

# maximal number of fixings found, until probing is interrupted (0: don't iterrupt)
# [type: int, range: [0,2147483647], default: 50]
presolving/probing/maxfixings = 50

# maximal number of successive probings without fixings, until probing is aborted (0: don't abort)
# [type: int, range: [0,2147483647], default: 2000]
presolving/probing/maxuseless = 2000

# maximal number of successive probings without fixings, bound changes, and implications, until probing is aborted (0: don't abort)
# [type: int, range: [0,2147483647], default: 100]
presolving/probing/maxtotaluseless = 100

# maximal number of probings without fixings, until probing is aborted (0: don't abort)
# [type: int, range: [0,2147483647], default: 0]
presolving/probing/maxsumuseless = 0

# priority of presolver <trivial>
# [type: int, range: [-536870912,536870911], default: 9000000]
presolving/trivial/priority = 9000000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/trivial/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/trivial/delay = FALSE

# priority of node selection rule <bfs> in standard mode
# [type: int, range: [-536870912,536870911], default: 100000]
nodeselection/bfs/stdpriority = 100000

# priority of node selection rule <bfs> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 0]
nodeselection/bfs/memsavepriority = 0

# minimal plunging depth, before new best node may be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/bfs/minplungedepth = -1

# maximal plunging depth, before new best node is forced to be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/bfs/maxplungedepth = -1

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where plunging is performed
# [type: real, range: [0,1.79769313486232e+308], default: 0.25]
nodeselection/bfs/maxplungequot = 0.25

# priority of node selection rule <dfs> in standard mode
# [type: int, range: [-536870912,536870911], default: 0]
nodeselection/dfs/stdpriority = 0

# priority of node selection rule <dfs> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 100000]
nodeselection/dfs/memsavepriority = 100000

# priority of node selection rule <estimate> in standard mode
# [type: int, range: [-536870912,536870911], default: 200000]
nodeselection/estimate/stdpriority = 200000

# priority of node selection rule <estimate> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 100]
nodeselection/estimate/memsavepriority = 100

# minimal plunging depth, before new best node may be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/estimate/minplungedepth = -1

# maximal plunging depth, before new best node is forced to be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/estimate/maxplungedepth = -1

# maximal quotient (estimate - lowerbound)/(cutoffbound - lowerbound) where plunging is performed
# [type: real, range: [0,1.79769313486232e+308], default: 0.25]
nodeselection/estimate/maxplungequot = 0.25

# frequency at which the best node instead of the best estimate is selected (0: never)
# [type: int, range: [0,2147483647], default: 10]
nodeselection/estimate/bestnodefreq = 10

# priority of node selection rule <hybridestim> in standard mode
# [type: int, range: [-536870912,536870911], default: 50000]
nodeselection/hybridestim/stdpriority = 50000

# priority of node selection rule <hybridestim> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 50]
nodeselection/hybridestim/memsavepriority = 50

# minimal plunging depth, before new best node may be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/hybridestim/minplungedepth = -1

# maximal plunging depth, before new best node is forced to be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/hybridestim/maxplungedepth = -1

# maximal quotient (estimate - lowerbound)/(cutoffbound - lowerbound) where plunging is performed
# [type: real, range: [0,1.79769313486232e+308], default: 0.25]
nodeselection/hybridestim/maxplungequot = 0.25

# frequency at which the best node instead of the hybrid best estimate / best bound is selected (0: never)
# [type: int, range: [0,2147483647], default: 1000]
nodeselection/hybridestim/bestnodefreq = 1000

# weight of estimate value in node selection score (0: pure best bound search, 1: pure best estimate search)
# [type: real, range: [0,1], default: 0.1]
nodeselection/hybridestim/estimweight = 0.1

# priority of node selection rule <restartdfs> in standard mode
# [type: int, range: [-536870912,536870911], default: 10000]
nodeselection/restartdfs/stdpriority = 50000

# priority of node selection rule <restartdfs> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 50000]
nodeselection/restartdfs/memsavepriority = 50000

# frequency for selecting the best node instead of the deepest one (0: never)
# [type: int, range: [0,2147483647], default: 1000]
nodeselection/restartdfs/selectbestfreq = 1000

# priority of branching rule <allfullstrong>
# [type: int, range: [-536870912,536870911], default: -1000]
branching/allfullstrong/priority = -1000

# maximal depth level, up to which branching rule <allfullstrong> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/allfullstrong/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/allfullstrong/maxbounddist = 1

# priority of branching rule <fullstrong>
# [type: int, range: [-536870912,536870911], default: 0]
branching/fullstrong/priority = 0

# maximal depth level, up to which branching rule <fullstrong> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/fullstrong/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/fullstrong/maxbounddist = 1

# number of intermediate LPs solved to trigger reevaluation of strong branching value for a variable that was already evaluated at the current node
# [type: int, range: [0,2147483647], default: 10]
branching/fullstrong/reevalage = 10

# priority of branching rule <inference>
# [type: int, range: [-536870912,536870911], default: 1000]
branching/inference/priority = 1000

# maximal depth level, up to which branching rule <inference> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/inference/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/inference/maxbounddist = 1

# factor to weigh conflict score against inference score
# [type: real, range: [0,1.79769313486232e+308], default: 1000]
branching/inference/conflictweight = 1000

# factor to weigh average number of cutoffs in branching score
# [type: real, range: [0,1.79769313486232e+308], default: 1]
branching/inference/cutoffweight = 1

# should branching on LP solution be restricted to the fractional variables?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
branching/inference/fractionals = TRUE

# priority of branching rule <mostinf>
# [type: int, range: [-536870912,536870911], default: 100]
branching/mostinf/priority = 100

# maximal depth level, up to which branching rule <mostinf> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/mostinf/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/mostinf/maxbounddist = 1

# priority of branching rule <leastinf>
# [type: int, range: [-536870912,536870911], default: 50]
branching/leastinf/priority = 50

# maximal depth level, up to which branching rule <leastinf> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/leastinf/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/leastinf/maxbounddist = 1

# priority of branching rule <pscost>
# [type: int, range: [-536870912,536870911], default: 2000]
branching/pscost/priority = 2000

# maximal depth level, up to which branching rule <pscost> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/pscost/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/pscost/maxbounddist = 1

# priority of branching rule <random>
# [type: int, range: [-536870912,536870911], default: -100000]
branching/random/priority = -100000

# maximal depth level, up to which branching rule <random> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/random/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/random/maxbounddist = 1

# priority of branching rule <relpscost>
# [type: int, range: [-536870912,536870911], default: 10000]
branching/relpscost/priority = 10000

# maximal depth level, up to which branching rule <relpscost> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/relpscost/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/relpscost/maxbounddist = 1

# weight in score calculations for conflict score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.01]
branching/relpscost/conflictweight = 0.01

# weight in score calculations for inference score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.0001]
branching/relpscost/inferenceweight = 0.0001

# weight in score calculations for cutoff score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.0001]
branching/relpscost/cutoffweight = 0.0001

# weight in score calculations for pseudo cost score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 1]
branching/relpscost/pscostweight = 1

# minimal value for minimum pseudo cost size to regard pseudo cost value as reliable
# [type: real, range: [0,1.79769313486232e+308], default: 1]
branching/relpscost/minreliable = 1

# maximal value for minimum pseudo cost size to regard pseudo cost value as reliable
# [type: real, range: [0,1.79769313486232e+308], default: 8]
branching/relpscost/maxreliable = 8

# maximal fraction of strong branching LP iterations compared to node relaxation LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
branching/relpscost/sbiterquot = 0.5

# additional number of allowed strong branching LP iterations
# [type: int, range: [0,2147483647], default: 100000]
branching/relpscost/sbiterofs = 100000

# maximal number of further variables evaluated without better score
# [type: int, range: [1,2147483647], default: 8]
branching/relpscost/maxlookahead = 8

# maximal number of candidates initialized with strong branching per node
# [type: int, range: [0,2147483647], default: 100]
branching/relpscost/initcand = 100

# iteration limit for strong branching initializations of pseudo cost entries (0: auto)
# [type: int, range: [0,2147483647], default: 0]
branching/relpscost/inititer = 0

# maximal number of bound tightenings before the node is reevaluated (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
branching/relpscost/maxbdchgs = 5

# priority of heuristic <actconsdiving>
# [type: int, range: [-536870912,536870911], default: -1003700]
heuristics/actconsdiving/priority = -1003700

# frequency for calling primal heuristic <actconsdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/actconsdiving/freq = -1

# frequency offset for calling primal heuristic <actconsdiving>
# [type: int, range: [0,2147483647], default: 5]
heuristics/actconsdiving/freqofs = 5

# maximal depth level to call primal heuristic <actconsdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/actconsdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/actconsdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/actconsdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/actconsdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/actconsdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/actconsdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/actconsdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/actconsdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/actconsdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/actconsdiving/backtrack = TRUE

# priority of heuristic <coefdiving>
# [type: int, range: [-536870912,536870911], default: -1001000]
heuristics/coefdiving/priority = -1001000

# frequency for calling primal heuristic <coefdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/coefdiving/freq = 10

# frequency offset for calling primal heuristic <coefdiving>
# [type: int, range: [0,2147483647], default: 1]
heuristics/coefdiving/freqofs = 1

# maximal depth level to call primal heuristic <coefdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/coefdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/coefdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/coefdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/coefdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/coefdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/coefdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/coefdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/coefdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/coefdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/coefdiving/backtrack = TRUE

# priority of heuristic <crossover>
# [type: int, range: [-536870912,536870911], default: -1011000]
heuristics/crossover/priority = -1011000

# frequency for calling primal heuristic <crossover> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 30]
heuristics/crossover/freq = 30

# frequency offset for calling primal heuristic <crossover>
# [type: int, range: [0,2147483647], default: 10]
heuristics/crossover/freqofs = 10

# maximal depth level to call primal heuristic <crossover> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/crossover/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/crossover/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/crossover/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/crossover/minnodes = 500

# number of solutions to be taken into account
# [type: int, range: [2,2147483647], default: 3]
heuristics/crossover/nusedsols = 3

# number of nodes without incumbent change that heuristic should wait
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/crossover/nwaitingnodes = 200

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/crossover/nodesquot = 0.1

# minimum percentage of integer variables that have to be fixed 
# [type: real, range: [0,1], default: 0.666]
heuristics/crossover/minfixingrate = 0.666

# factor by which Crossover should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/crossover/minimprove = 0.01

# should the choice which sols to take be randomized?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/crossover/randomization = TRUE

# priority of heuristic <feaspump>
# [type: int, range: [-536870912,536870911], default: -1000000]
heuristics/feaspump/priority = -1000000

# frequency for calling primal heuristic <feaspump> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/feaspump/freq = 20

# frequency offset for calling primal heuristic <feaspump>
# [type: int, range: [0,2147483647], default: 0]
heuristics/feaspump/freqofs = 0

# maximal depth level to call primal heuristic <feaspump> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/feaspump/maxdepth = -1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.01]
heuristics/feaspump/maxlpiterquot = 0.01

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/feaspump/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: 2]
heuristics/feaspump/maxsols = 5

# factor by which the regard of the objective is decreased in each round, 1.0 for dynamic
# [type: real, range: [0,1], default: 1]
heuristics/feaspump/objfactor = 1

# maximal number of pumping loops (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10000]
heuristics/feaspump/maxloops = 10000

# maximal number of pumping rounds without fractionality improvement (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/feaspump/maxstallloops = 10

# minimum number of random variables to flip, if a 1-cycle is encountered
# [type: int, range: [1,2147483647], default: 10]
heuristics/feaspump/minflips = 10

# maximum length of cycles to be checked explicitly in each round
# [type: int, range: [1,100], default: 3]
heuristics/feaspump/cyclelength = 3

# number of iterations until a random perturbation is forced
# [type: int, range: [1,2147483647], default: 100]
heuristics/feaspump/perturbfreq = 100

# should the feasibility pump be called at root node before cut separation?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/feaspump/beforecuts = FALSE

# priority of heuristic <fixandinfer>
# [type: int, range: [-536870912,536870911], default: -500000]
heuristics/fixandinfer/priority = -500000

# frequency for calling primal heuristic <fixandinfer> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/fixandinfer/freq = -1

# frequency offset for calling primal heuristic <fixandinfer>
# [type: int, range: [0,2147483647], default: 0]
heuristics/fixandinfer/freqofs = 0

# maximal depth level to call primal heuristic <fixandinfer> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/fixandinfer/maxdepth = -1

# maximal number of propagation rounds in probing subproblems (-1: no limit, 0: auto)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/fixandinfer/proprounds = 0

# minimal number of fixings to apply before dive may be aborted
# [type: int, range: [0,2147483647], default: 100]
heuristics/fixandinfer/minfixings = 100

# priority of heuristic <fracdiving>
# [type: int, range: [-536870912,536870911], default: -1003000]
heuristics/fracdiving/priority = -1003000

# frequency for calling primal heuristic <fracdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/fracdiving/freq = 10

# frequency offset for calling primal heuristic <fracdiving>
# [type: int, range: [0,2147483647], default: 3]
heuristics/fracdiving/freqofs = 3

# maximal depth level to call primal heuristic <fracdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/fracdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/fracdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/fracdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/fracdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/fracdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/fracdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/fracdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/fracdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/fracdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/fracdiving/backtrack = TRUE

# priority of heuristic <guideddiving>
# [type: int, range: [-536870912,536870911], default: -1007000]
heuristics/guideddiving/priority = -1007000

# frequency for calling primal heuristic <guideddiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/guideddiving/freq = 10

# frequency offset for calling primal heuristic <guideddiving>
# [type: int, range: [0,2147483647], default: 7]
heuristics/guideddiving/freqofs = 7

# maximal depth level to call primal heuristic <guideddiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/guideddiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/guideddiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/guideddiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/guideddiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/guideddiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/guideddiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/guideddiving/maxdiveavgquot = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/guideddiving/backtrack = TRUE

# priority of heuristic <intdiving>
# [type: int, range: [-536870912,536870911], default: -1003500]
heuristics/intdiving/priority = -1003500

# frequency for calling primal heuristic <intdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/intdiving/freq = -1

# frequency offset for calling primal heuristic <intdiving>
# [type: int, range: [0,2147483647], default: 9]
heuristics/intdiving/freqofs = 0

# maximal depth level to call primal heuristic <intdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/intdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/intdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/intdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/intdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/intdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/intdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/intdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/intdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/intdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/intdiving/backtrack = TRUE

# priority of heuristic <intshifting>
# [type: int, range: [-536870912,536870911], default: -10000]
heuristics/intshifting/priority = -10000

# frequency for calling primal heuristic <intshifting> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/intshifting/freq = 10

# frequency offset for calling primal heuristic <intshifting>
# [type: int, range: [0,2147483647], default: 0]
heuristics/intshifting/freqofs = 0

# maximal depth level to call primal heuristic <intshifting> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/intshifting/maxdepth = -1

# priority of heuristic <linesearchdiving>
# [type: int, range: [-536870912,536870911], default: -1006000]
heuristics/linesearchdiving/priority = -1006000

# frequency for calling primal heuristic <linesearchdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/linesearchdiving/freq = 10

# frequency offset for calling primal heuristic <linesearchdiving>
# [type: int, range: [0,2147483647], default: 6]
heuristics/linesearchdiving/freqofs = 6

# maximal depth level to call primal heuristic <linesearchdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/linesearchdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/linesearchdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/linesearchdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/linesearchdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/linesearchdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/linesearchdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/linesearchdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/linesearchdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/linesearchdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/linesearchdiving/backtrack = TRUE

# priority of heuristic <localbranching>
# [type: int, range: [-536870912,536870911], default: -1010000]
heuristics/localbranching/priority = -1010000

# frequency for calling primal heuristic <localbranching> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/localbranching/freq = -1

# frequency offset for calling primal heuristic <localbranching>
# [type: int, range: [0,2147483647], default: 0]
heuristics/localbranching/freqofs = 0

# maximal depth level to call primal heuristic <localbranching> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/localbranching/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 1000]
heuristics/localbranching/nodesofs = 1000

# radius (using Manhattan metric) of the incumbent's neighborhood to be searched
# [type: int, range: [1,2147483647], default: 18]
heuristics/localbranching/neighborhoodsize = 18

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.05]
heuristics/localbranching/nodesquot = 0.05

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 1000]
heuristics/localbranching/minnodes = 1000

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 10000]
heuristics/localbranching/maxnodes = 10000

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/localbranching/nwaitingnodes = 200

# factor by which localbranching should at least improve the incumbent  
# [type: real, range: [0,1], default: 0.01]
heuristics/localbranching/minimprove = 0.01

# priority of heuristic <mutation>
# [type: int, range: [-536870912,536870911], default: -1020000]
heuristics/mutation/priority = -1020000

# frequency for calling primal heuristic <mutation> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/mutation/freq = -1

# frequency offset for calling primal heuristic <mutation>
# [type: int, range: [0,2147483647], default: 8]
heuristics/mutation/freqofs = 8

# maximal depth level to call primal heuristic <mutation> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/mutation/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 500]
heuristics/mutation/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 5000]
heuristics/mutation/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 500]
heuristics/mutation/minnodes = 500

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/mutation/nwaitingnodes = 200

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/mutation/nodesquot = 0.1

# percentage of integer variables that have to be fixed 
# [type: real, range: [1e-06,0.999999], default: 0.8]
heuristics/mutation/minfixingrate = 0.8

# factor by which Mutation should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/mutation/minimprove = 0.01

# priority of heuristic <objpscostdiving>
# [type: int, range: [-536870912,536870911], default: -1004000]
heuristics/objpscostdiving/priority = -1004000

# frequency for calling primal heuristic <objpscostdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/objpscostdiving/freq = 20

# frequency offset for calling primal heuristic <objpscostdiving>
# [type: int, range: [0,2147483647], default: 4]
heuristics/objpscostdiving/freqofs = 4

# maximal depth level to call primal heuristic <objpscostdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/objpscostdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/objpscostdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/objpscostdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to total iteration number
# [type: real, range: [0,1], default: 0.01]
heuristics/objpscostdiving/maxlpiterquot = 0.01

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/objpscostdiving/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/objpscostdiving/maxsols = -1

# maximal diving depth: number of binary/integer variables times depthfac
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
heuristics/objpscostdiving/depthfac = 0.5

# maximal diving depth factor if no feasible solution was found yet
# [type: real, range: [0,1.79769313486232e+308], default: 2]
heuristics/objpscostdiving/depthfacnosol = 2

# priority of heuristic <octane>
# [type: int, range: [-536870912,536870911], default: -1008000]
heuristics/octane/priority = -1008000

# frequency for calling primal heuristic <octane> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/octane/freq = 10

# frequency offset for calling primal heuristic <octane>
# [type: int, range: [0,2147483647], default: 0]
heuristics/octane/freqofs = 0

# maximal depth level to call primal heuristic <octane> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/octane/maxdepth = -1

# number of 0-1-points to be tested as possible solutions by OCTANE
# [type: int, range: [1,2147483647], default: 100]
heuristics/octane/fmax = 100

# number of 0-1-points to be tested at first whether they violate a common row
# [type: int, range: [1,2147483647], default: 10]
heuristics/octane/ffirst = 10

# execute OCTANE only in the space of fractional variables (TRUE) or in the full space? 
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/usefracspace = TRUE

# should the inner normal of the objective be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useobjray = TRUE

# should the average of the basic cone be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useavgray = TRUE

# should the difference between the root solution and the current LP solution be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/octane/usediffray = TRUE

# should the weighted average of the basic cone be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useavgwgtray = TRUE

# should the weighted average of the nonbasic cone be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useavgnbray = TRUE

# priority of heuristic <oneopt>
# [type: int, range: [-536870912,536870911], default: -20000]
heuristics/oneopt/priority = -20000

# frequency for calling primal heuristic <oneopt> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/oneopt/freq = 1

# frequency offset for calling primal heuristic <oneopt>
# [type: int, range: [0,2147483647], default: 0]
heuristics/oneopt/freqofs = 0

# maximal depth level to call primal heuristic <oneopt> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/oneopt/maxdepth = -1

# should the objective be weighted with the potential shifting value when sorting the shifting candidates?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/oneopt/weightedobj = TRUE

# priority of heuristic <pscostdiving>
# [type: int, range: [-536870912,536870911], default: -1002000]
heuristics/pscostdiving/priority = -1002000

# frequency for calling primal heuristic <pscostdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/pscostdiving/freq = 10

# frequency offset for calling primal heuristic <pscostdiving>
# [type: int, range: [0,2147483647], default: 2]
heuristics/pscostdiving/freqofs = 2

# maximal depth level to call primal heuristic <pscostdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/pscostdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/pscostdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/pscostdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/pscostdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/pscostdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/pscostdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/pscostdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/pscostdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/pscostdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/pscostdiving/backtrack = TRUE

# priority of heuristic <rens>
# [type: int, range: [-536870912,536870911], default: -1100000]
heuristics/rens/priority = -1100000

# frequency for calling primal heuristic <rens> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/rens/freq = 0

# frequency offset for calling primal heuristic <rens>
# [type: int, range: [0,2147483647], default: 0]
heuristics/rens/freqofs = 0

# maximal depth level to call primal heuristic <rens> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rens/maxdepth = -1

# minimum percentage of integer variables that have to be fixable 
# [type: real, range: [0,1], default: 0.5]
heuristics/rens/minfixingrate = 0.5

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/rens/maxnodes = 5000

# should general integers get binary bounds [floor(.),ceil(.)] ?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/rens/binarybounds = TRUE

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/rens/nodesofs = 500

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/rens/minnodes = 500

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/rens/nodesquot = 0.1

# factor by which RENS should at least improve the incumbent  
# [type: real, range: [0,1], default: 0.01]
heuristics/rens/minimprove = 0.01

# priority of heuristic <rins>
# [type: int, range: [-536870912,536870911], default: -1009000]
heuristics/rins/priority = -1009000

# frequency for calling primal heuristic <rins> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rins/freq = -1

# frequency offset for calling primal heuristic <rins>
# [type: int, range: [0,2147483647], default: 5]
heuristics/rins/freqofs = 5

# maximal depth level to call primal heuristic <rins> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rins/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 500]
heuristics/rins/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 5000]
heuristics/rins/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 500]
heuristics/rins/minnodes = 500

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/rins/nodesquot = 0.1

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/rins/nwaitingnodes = 200

# factor by which RINS should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/rins/minimprove = 0.01

# minimum percentage of integer variables that have to be fixed 
# [type: real, range: [0,1], default: 0]
heuristics/rins/minfixingrate = 0

# priority of heuristic <rootsoldiving>
# [type: int, range: [-536870912,536870911], default: -1005000]
heuristics/rootsoldiving/priority = -1005000

# frequency for calling primal heuristic <rootsoldiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/rootsoldiving/freq = 20

# frequency offset for calling primal heuristic <rootsoldiving>
# [type: int, range: [0,2147483647], default: 5]
heuristics/rootsoldiving/freqofs = 5

# maximal depth level to call primal heuristic <rootsoldiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rootsoldiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/rootsoldiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/rootsoldiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.01]
heuristics/rootsoldiving/maxlpiterquot = 0.01

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/rootsoldiving/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rootsoldiving/maxsols = -1

# maximal diving depth: number of binary/integer variables times depthfac
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
heuristics/rootsoldiving/depthfac = 0.5

# maximal diving depth factor if no feasible solution was found yet
# [type: real, range: [0,1.79769313486232e+308], default: 2]
heuristics/rootsoldiving/depthfacnosol = 2

# priority of heuristic <rounding>
# [type: int, range: [-536870912,536870911], default: -1000]
heuristics/rounding/priority = -1000

# frequency for calling primal heuristic <rounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/rounding/freq = 1

# frequency offset for calling primal heuristic <rounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/rounding/freqofs = 0

# maximal depth level to call primal heuristic <rounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rounding/maxdepth = -1

# priority of heuristic <shifting>
# [type: int, range: [-536870912,536870911], default: -5000]
heuristics/shifting/priority = -5000

# frequency for calling primal heuristic <shifting> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/shifting/freq = 10

# frequency offset for calling primal heuristic <shifting>
# [type: int, range: [0,2147483647], default: 0]
heuristics/shifting/freqofs = 0

# maximal depth level to call primal heuristic <shifting> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/shifting/maxdepth = -1

# priority of heuristic <simplerounding>
# [type: int, range: [-536870912,536870911], default: 0]
heuristics/simplerounding/priority = 0

# frequency for calling primal heuristic <simplerounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/simplerounding/freq = 1

# frequency offset for calling primal heuristic <simplerounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/simplerounding/freqofs = 0

# maximal depth level to call primal heuristic <simplerounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/simplerounding/maxdepth = -1

# priority of heuristic <veclendiving>
# [type: int, range: [-536870912,536870911], default: -1003100]
heuristics/veclendiving/priority = -1003100

# frequency for calling primal heuristic <veclendiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/veclendiving/freq = 10

# frequency offset for calling primal heuristic <veclendiving>
# [type: int, range: [0,2147483647], default: 4]
heuristics/veclendiving/freqofs = 4

# maximal depth level to call primal heuristic <veclendiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/veclendiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/veclendiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/veclendiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/veclendiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/veclendiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/veclendiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/veclendiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/veclendiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/veclendiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/veclendiving/backtrack = TRUE

# priority of propagator <pseudoobj>
# [type: int, range: [-536870912,536870911], default: 0]
propagating/pseudoobj/priority = 0

# frequency for calling propagator <pseudoobj> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/pseudoobj/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/pseudoobj/delay = FALSE

# maximal number of variables to look at in a single propagation round (-1: process all variables)
# [type: int, range: [-1,2147483647], default: 100]
propagating/pseudoobj/maxcands = 100

# priority of propagator <rootredcost>
# [type: int, range: [-536870912,536870911], default: 1000000]
propagating/rootredcost/priority = 1000000

# frequency for calling propagator <rootredcost> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/rootredcost/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/rootredcost/delay = FALSE

# priority of separator <clique>
# [type: int, range: [-536870912,536870911], default: -5000]
separating/clique/priority = -5000

# frequency for calling separator <clique> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/clique/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <clique> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/clique/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/clique/delay = FALSE

# factor for scaling weights
# [type: real, range: [1,1.79769313486232e+308], default: 1000]
separating/clique/scaleval = 1000

# maximal number of nodes in branch and bound tree (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
separating/clique/maxtreenodes = -1

# frequency for premature backtracking up to tree level 1 (0: no backtracking)
# [type: int, range: [0,2147483647], default: 10000]
separating/clique/backtrackfreq = 10000

# maximal number of clique cuts separated per separation round (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
separating/clique/maxsepacuts = 10

# maximal number of zero-valued variables extending the clique (-1: no limit)
# [type: int, range: [-1,2147483647], default: 1000]
separating/clique/maxzeroextensions = 1000

# maximal memory size of dense clique table (in kb)
# [type: real, range: [0,2097151.99902344], default: 20000]
separating/clique/cliquetablemem = 20000

# minimal density of cliques to use a dense clique table
# [type: real, range: [0,1], default: 0.05]
separating/clique/cliquedensity = 0.05

# priority of separator <cmir>
# [type: int, range: [-536870912,536870911], default: -3000]
separating/cmir/priority = -3000

# frequency for calling separator <cmir> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/cmir/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <cmir> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/cmir/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/cmir/delay = FALSE

# maximal number of cmir separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 3]
separating/cmir/maxrounds = 3

# maximal number of cmir separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/cmir/maxroundsroot = 10

# maximal number of rows to start aggregation with per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/cmir/maxtries = 100

# maximal number of rows to start aggregation with per separation round in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/cmir/maxtriesroot = -1

# maximal number of consecutive unsuccesful aggregation tries (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 20]
separating/cmir/maxfails = 20

# maximal number of consecutive unsuccesful aggregation tries in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/cmir/maxfailsroot = 100

# maximal number of aggregations for each row per separation round
# [type: int, range: [0,2147483647], default: 3]
separating/cmir/maxaggrs = 3

# maximal number of aggregations for each row per separation round in the root node
# [type: int, range: [0,2147483647], default: 6]
separating/cmir/maxaggrsroot = 6

# maximal number of cmir cuts separated per separation round
# [type: int, range: [0,2147483647], default: 100]
separating/cmir/maxsepacuts = 50

# maximal number of cmir cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 500]
separating/cmir/maxsepacutsroot = 500

# maximal slack of rows to be used in aggregation
# [type: real, range: [0,1.79769313486232e+308], default: 0]
separating/cmir/maxslack = 0

# maximal slack of rows to be used in aggregation in the root node
# [type: real, range: [0,1.79769313486232e+308], default: 0.1]
separating/cmir/maxslackroot = 0.1

# weight of row density in the aggregation scoring of the rows
# [type: real, range: [0,1.79769313486232e+308], default: 0.0001]
separating/cmir/densityscore = 0.0001

# weight of slack in the aggregation scoring of the rows
# [type: real, range: [0,1.79769313486232e+308], default: 0.001]
separating/cmir/slackscore = 0.001

# maximal density of aggregated row
# [type: real, range: [0,1], default: 0.2]
separating/cmir/maxaggdensity = 0.2

# maximal density of row to be used in aggregation
# [type: real, range: [0,1], default: 0.05]
separating/cmir/maxrowdensity = 0.05

# additional number of variables allowed in row on top of density
# [type: int, range: [0,2147483647], default: 100]
separating/cmir/densityoffset = 100

# maximal row aggregation factor
# [type: real, range: [0,1.79769313486232e+308], default: 10000]
separating/cmir/maxrowfac = 10000

# maximal number of different deltas to try (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/cmir/maxtestdelta = 20

# maximal number of active continuous variables in aggregated row
# [type: int, range: [0,2147483647], default: 10]
separating/cmir/maxconts = 10

# maximal number of active continuous variables in aggregated row in the root node
# [type: int, range: [0,2147483647], default: 10]
separating/cmir/maxcontsroot = 10

# tolerance for bounddistances used to select continuous variable in current aggregated constraint to be eliminated
# [type: real, range: [0,1.79769313486232e+308], default: 0.1]
separating/cmir/aggrtol = 0.1

# should negative values also be tested in scaling?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/cmir/trynegscaling = TRUE

# should an additional variable be complemented if f0 = 0?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/cmir/fixintegralrhs = TRUE

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/cmir/dynamiccuts = TRUE

# priority of separator <flowcover>
# [type: int, range: [-536870912,536870911], default: -4000]
separating/flowcover/priority = -4000

# frequency for calling separator <flowcover> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/flowcover/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <flowcover> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/flowcover/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/flowcover/delay = FALSE

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/flowcover/maxrounds = 5

# maximal number of separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/flowcover/maxroundsroot = 10

# maximal number of rows to separate flow cover cuts for per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/flowcover/maxtries = 100

# maximal number of rows to separate flow cover cuts for per separation round in the root (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/flowcover/maxtriesroot = -1

# maximal number of consecutive fails to generate a cut per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 50]
separating/flowcover/maxfails = 50

# maximal number of consecutive fails to generate a cut per separation round in the root (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/flowcover/maxfailsroot = 100

# maximal number of flow cover cuts separated per separation round
# [type: int, range: [0,2147483647], default: 100]
separating/flowcover/maxsepacuts = 100

# maximal number of flow cover cuts separated per separation round in the root
# [type: int, range: [0,2147483647], default: 200]
separating/flowcover/maxsepacutsroot = 200

# maximal slack of rows to separate flow cover cuts for
# [type: real, range: [0,1.79769313486232e+308], default: 1.79769313486232e+308]
separating/flowcover/maxslack = 1.79769313486232e+308

# maximal slack of rows to separate flow cover cuts for in the root
# [type: real, range: [0,1.79769313486232e+308], default: 1.79769313486232e+308]
separating/flowcover/maxslackroot = 1.79769313486232e+308

# weight of slack in the scoring of the rows
# [type: real, range: [0,1.79769313486232e+308], default: 0.001]
separating/flowcover/slackscore = 0.001

# maximal density of row to separate flow cover cuts for
# [type: real, range: [0,1], default: 1]
separating/flowcover/maxrowdensity = 1

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/flowcover/dynamiccuts = TRUE

# should flow cover cuts be separated for 0-1 single node flow set with reversed arcs in addition?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/flowcover/multbyminusone = TRUE

# cut generation heuristic: maximal number of different deltas to try
# [type: int, range: [0,2147483647], default: 10]
separating/flowcover/maxtestdelta = 10

# priority of separator <gomory>
# [type: int, range: [-536870912,536870911], default: -1000]
separating/gomory/priority = -1000

# frequency for calling separator <gomory> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/gomory/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <gomory> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/gomory/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/gomory/delay = FALSE

# maximal number of gomory separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/gomory/maxrounds = 5

# maximal number of gomory separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/gomory/maxroundsroot = -1

# maximal number of gomory cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
separating/gomory/maxsepacuts = 50

# maximal number of gomory cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 500]
separating/gomory/maxsepacutsroot = 500

# maximal valid range max(|weights|)/min(|weights|) of row weights
# [type: real, range: [1,1.79769313486232e+308], default: 10000]
separating/gomory/maxweightrange = 10000

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/gomory/dynamiccuts = TRUE

# priority of separator <impliedbounds>
# [type: int, range: [-536870912,536870911], default: -50]
separating/impliedbounds/priority = -50

# frequency for calling separator <impliedbounds> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/impliedbounds/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <impliedbounds> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/impliedbounds/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/impliedbounds/delay = FALSE

# priority of separator <intobj>
# [type: int, range: [-536870912,536870911], default: -100]
separating/intobj/priority = -100

# frequency for calling separator <intobj> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
separating/intobj/freq = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <intobj> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/intobj/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/intobj/delay = FALSE

# priority of separator <redcost>
# [type: int, range: [-536870912,536870911], default: 10000000]
separating/redcost/priority = 10000000

# frequency for calling separator <redcost> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
separating/redcost/freq = 1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <redcost> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
separating/redcost/maxbounddist = 1

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/redcost/delay = FALSE

# should reduced cost fixing be also applied to continuous variables?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/redcost/continuous = FALSE

# priority of separator <strongcg>
# [type: int, range: [-536870912,536870911], default: -2000]
separating/strongcg/priority = -2000

# frequency for calling separator <strongcg> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/strongcg/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <strongcg> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/strongcg/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/strongcg/delay = FALSE

# maximal number of strong CG separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/strongcg/maxrounds = 5

# maximal number of strong CG separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 20]
separating/strongcg/maxroundsroot = 20

# maximal number of strong CG cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
separating/strongcg/maxsepacuts = 50

# maximal number of strong CG cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 500]
separating/strongcg/maxsepacutsroot = 500

# maximal valid range max(|weights|)/min(|weights|) of row weights
# [type: real, range: [1,1.79769313486232e+308], default: 10000]
separating/strongcg/maxweightrange = 10000

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/strongcg/dynamiccuts = TRUE

# display activation status of display column <solfound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/solfound/active = 1

# display activation status of display column <time> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/time/active = 1

# display activation status of display column <nnodes> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nnodes/active = 1

# display activation status of display column <nodesleft> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nodesleft/active = 1

# display activation status of display column <lpiterations> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/lpiterations/active = 1

# display activation status of display column <memused> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/memused/active = 1

# display activation status of display column <depth> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/depth/active = 1

# display activation status of display column <maxdepth> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/maxdepth/active = 1

# display activation status of display column <plungedepth> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/plungedepth/active = 1

# display activation status of display column <nfrac> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nfrac/active = 1

# display activation status of display column <vars> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/vars/active = 1

# display activation status of display column <conss> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/conss/active = 1

# display activation status of display column <curconss> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/curconss/active = 1

# display activation status of display column <curcols> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/curcols/active = 1

# display activation status of display column <currows> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/currows/active = 1

# display activation status of display column <cuts> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/cuts/active = 1

# display activation status of display column <separounds> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/separounds/active = 1

# display activation status of display column <poolsize> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/poolsize/active = 1

# display activation status of display column <conflicts> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/conflicts/active = 1

# display activation status of display column <strongbranchs> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/strongbranchs/active = 1

# display activation status of display column <lpobj> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/lpobj/active = 1

# display activation status of display column <curdualbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/curdualbound/active = 1

# display activation status of display column <estimate> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/estimate/active = 1

# display activation status of display column <avgdualbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/avgdualbound/active = 1

# display activation status of display column <dualbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/dualbound/active = 1

# display activation status of display column <primalbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/primalbound/active = 1

# display activation status of display column <cutoffbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/cutoffbound/active = 1

# display activation status of display column <gap> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/gap/active = 1

# display activation status of display column <nsols> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nsols/active = 1

